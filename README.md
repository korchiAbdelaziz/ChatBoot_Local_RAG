
````md
# ğŸ¤– RAG Neural Chat â€” Streamlit + Ollama

RAG Neural Chat est une application **Retrieval-Augmented Generation (RAG)** qui combine :
- ğŸ§  une **base de connaissances locale** (PDF, TXT, MD)
- ğŸ” une **recherche sÃ©mantique**
- ğŸ¤– un **LLM local via Ollama**
- ğŸ¨ une interface **Streamlit cyber-style**

---

## ğŸ–¼ï¸ AperÃ§u de lâ€™application

![RAG Neural Chat UI](assets/Capture_Ecrant_Rag.png)


---

## ğŸš€ FonctionnalitÃ©s

- ğŸ“„ Upload de documents (PDF / TXT / Markdown)
- ğŸ§© Indexation locale des documents
- ğŸ” Recherche contextuelle (RAG)
- ğŸ¤– GÃ©nÃ©ration de rÃ©ponses avec **Ollama**
- ğŸ’¬ Interface chat moderne et animÃ©e
- âš™ï¸ Choix dynamique du modÃ¨le Ollama

---

## ğŸ—‚ï¸ Structure du projet


## âš™ï¸ Installation

### 1ï¸âƒ£ Cloner le projet

### 2ï¸âƒ£ CrÃ©er un environnement virtuel

```bash
python -m venv venv
```

Activer lâ€™environnement :

**Windows**

```bash
venv\Scripts\activate
```

**Linux / macOS**

```bash
source venv/bin/activate
```

---

### 3ï¸âƒ£ Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

---

## ğŸ¤– Installation et dÃ©marrage dâ€™Ollama

### Installer Ollama

TÃ©lÃ©charger Ollama depuis le site officiel :
ğŸ‘‰ [https://ollama.com](https://ollama.com)

---

### TÃ©lÃ©charger un modÃ¨le LLM

```bash
ollama pull llama3.2
```

---

### Lancer Ollama

```bash
ollama serve
```

âœ… Ollama doit Ãªtre accessible sur :

```
http://localhost:11434
```

---

## â–¶ï¸ DÃ©marrer lâ€™application RAG

Une fois lâ€™environnement activÃ© **et Ollama lancÃ©** :

```bash
streamlit run Rag_app.py
```

Puis ouvrir dans le navigateur :

```
http://localhost:8501
```

---

## ğŸ“„ Utilisation

1. ğŸ“¤ Importer un document (PDF / TXT / MD) depuis la **sidebar**
2. ğŸ§  Cliquer sur **Indexer le cerveau** pour gÃ©nÃ©rer les embeddings
3. ğŸ’¬ Poser une question dans le champ de chat
4. ğŸ¤– Le modÃ¨le Ollama rÃ©pond en utilisant la **base de connaissances locale**

---

## ğŸ§  Principe RAG (RÃ©sumÃ©)

```text
Question utilisateur
        â†“
Recherche sÃ©mantique (embeddings)
        â†“
Documents pertinents
        â†“
Prompt enrichi
        â†“
LLM local (Ollama)
```

---

## ğŸ› ï¸ ProblÃ¨mes courants

### âŒ Erreur : `ModuleNotFoundError: numpy._core`

â¡ï¸ Supprimer le fichier `embeddings.pkl`
â¡ï¸ RÃ©installer NumPy :

```bash
pip install numpy==1.26.4
```

---

### âŒ Ollama ne rÃ©pond pas

VÃ©rifier que le service est actif :

```
http://localhost:11434
```

---

## ğŸ“Œ AmÃ©liorations possibles

* ğŸ” Authentification utilisateur
* ğŸ“š Multi-base de documents
* ğŸ§  MÃ©moire longue durÃ©e
* ğŸŒ DÃ©ploiement cloud
* ğŸ—ƒï¸ Support Word / HTML

---

## ğŸ‘¨â€ğŸ’» Auteur

**Abdelaziz Korchi**
Projet acadÃ©mique â€” Deep Learning & RAG

---


